這個檔案包含22_RNN_Train.py所有細節:


RNN（循環神經網絡）的輸入通常是一排向量，這些向量可以是序列中的每個時間步的特徵表示。
RNN可以處理變長的序列輸入，並能夠根據序列的上下文進行學習和預測。

1. 第267 ~ 294行中:
=> 因為RNN的輸入是一排向量，所以這段程式碼，將輸入轉為一維向量


2. 第298 ~ 306行中:
=> Embedding 是將高維度的稀疏向量（例如單詞的one-hot編碼）轉換為低維度的密集向量的一種技術。
    它的主要目的是捕捉和學習數據中隱含的語義和關聯。
    簡單來說就是把詞彙轉換為數值向量，方便尋找詞彙關聯性。

3. 第309 ~ 310行中:
=> GRU（門控循環單元，Gated Recurrent Unit）是一種改進版的循環神經網絡（RNN）結構，用於處理序列數據。
    GRU層的主要功能是捕捉序列中的長期依賴性並改善傳統RNN在學習長期依賴時遇到的梯度消失問題。

4. 第314 ~ 325行中:
=> 是以Functional API的方式將所有神經層連接起來最為輸入層
=> 再透過第328 ~ 331行，不斷連接activation 並Dropout神經層的動作。
=> 最後再把第334行中輸出層加進來就完成，整個模型了。

5. 第37 ~ 39行中:
=> 是定義模型的輸出和輸入型式。

6.必須將這些項目（name, item_desc, brand_name, item_condition, num_vars, desc_len, name_len, subcat_0, subcat_1, subcat_2）
經過嵌入層（Embedding Layer）和GRU層等處理，才能形成神經層，並進行進一步的特徵提取和預測。
=> 進而打包成為一個輸入層。

7. 而第337 ~ 339行中:
=> 則是定義輸入神經層的神經(有點類似接口)。