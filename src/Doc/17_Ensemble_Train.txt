這個檔案包含17_Ensemble_Train.py所有細節:

整體程式碼和16_Ensemble.py差不多
只不過Ensemble技術從多數決Ensemble變成平均Ensemble。

1. 我很好奇create 5個一模一樣的模型，然後以平均集成方式作為預測結果有比較好嗎?
不應該所有模型的預測結果都一樣嗎?

Ans: 
即使是五個看似相同的模型，它們的預測結果可能仍然會有所不同。
以下是一些原因和情境解釋為什麼多個模型的預測結果不會完全一致，即使它們的結構和訓練過程完全相同:

為什麼預測結果可能不同
=> 隨機初始化：
    模型的權重在訓練開始時通常會進行隨機初始化。
    即使模型架構相同，這些隨機權重初始化會導致每個模型的訓練結果有所不同。

=> 數據的隨機性：
    在訓練過程中，數據的隨機抽樣（如批次選擇、數據增強）會影響每個模型的學習過程。
    即使使用相同的數據集，這些隨機因素也會使每個模型學習到略有不同的特徵。

=> 隨機性在訓練過程中：
    訓練過程中的隨機性，如隨機梯度下降中的隨機批次選擇，也會導致不同模型在相同訓練數據上的學習結果有所不同。


***平均集成的好處
即使五個模型的預測結果不完全相同，使用平均集成方法仍然可以有以下好處：
=> 降低過擬合：
    多個模型的預測結果通過平均可以減少個別模型可能存在的過擬合問題。
    集成方法可以平衡不同模型的預測，從而提高整體模型的泛化能力。

=> 減少預測變異性：
    通過平均多個模型的預測結果，可以減少單個模型預測的變異性，使最終的預測結果更加穩定和可靠。

=> 捕捉不同模型的特徵：
    即使模型結構相同，但由於訓練過程中的隨機性，每個模型可能會學習到數據的不同方面。
    通過集成，可以綜合不同模型的學習特徵，提高預測的全面性。

(***泛化能力（Generalization）是機器學習模型的一個重要概念，
    指的是模型對於未見過的數據或新數據的預測能力。***)
